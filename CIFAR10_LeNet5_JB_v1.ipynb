{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprieton/CIFAR10_LeNet5/blob/main/CIFAR10_LeNet5_JB_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfsExmCkl3-9"
      },
      "source": [
        "# Project I: Understanding Calibration in CNNs\n",
        "\n",
        "En este proyecto, investigaremos la calibración de las Redes Neuronales Convolucionales (CNNs) en un entorno de clasificación binaria.\n",
        "\n",
        "**Objetivos:**\n",
        "1.  Entrenar una CNN LeNet-5 desde cero para clasificar **pájaros** vs. **gatos** del dataset CIFAR-10.\n",
        "2.  Evaluar la calibración del modelo utilizando diagramas de fiabilidad y el Expected Calibration Error (ECE).\n",
        "3.  Implementar y evaluar el método de *Temperature Scaling* para mejorar la calibración del modelo.\n",
        "4.  (Opcional) Repetir el experimento con un modelo pre-entrenado más grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJGO2Xvjl3_A"
      },
      "source": [
        "## 1. Preparación del Entorno\n",
        "\n",
        "Primero, importamos todas las librerías necesarias para el proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ2SLVDcl3_C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Configuración del dispositivo (GPU o CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO809OhGl3_H"
      },
      "source": [
        "## 2. Carga y Preprocesamiento de Datos (CIFAR-10)\n",
        "\n",
        "Cargaremos el dataset CIFAR-10, pero solo nos interesan dos clases: **gatos** (clase 3) y **pájaros** (clase 2). Filtraremos el dataset para quedarnos únicamente con estas dos clases y re-etiquetaremos las clases a 0 (pájaro) y 1 (gato) para nuestro problema de clasificación binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZa5v_lTl3_H"
      },
      "outputs": [],
      "source": [
        "# Transformaciones para las imágenes de CIFAR-10\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Descargar y cargar el dataset de entrenamiento\n",
        "trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# Descargar y cargar el dataset de test\n",
        "testset_full = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Clases que nos interesan: 2 (bird) y 3 (cat)\n",
        "classes_to_keep = [2, 3]\n",
        "\n",
        "def filter_dataset(dataset, classes):\n",
        "    # Extraer índices de las clases que queremos mantener\n",
        "    idx = [i for i, (img, label) in enumerate(dataset) if label in classes]\n",
        "    # Crear un nuevo mapeo de etiquetas: bird (2) -> 0, cat (3) -> 1\n",
        "    dataset.targets = [0 if dataset.targets[i] == classes[0] else 1 for i in idx]\n",
        "    # Crear el subconjunto del dataset\n",
        "    return Subset(dataset, idx)\n",
        "\n",
        "# Filtrar los datasets\n",
        "trainset_filtered = filter_dataset(trainset_full, classes_to_keep)\n",
        "testset = filter_dataset(testset_full, classes_to_keep)\n",
        "\n",
        "# Dividir el conjunto de entrenamiento filtrado en entrenamiento y validación (80/20)\n",
        "# Usaremos el set de validación para encontrar el parámetro 'a' de Temperature Scaling\n",
        "train_size = int(0.8 * len(trainset_filtered))\n",
        "val_size = len(trainset_filtered) - train_size\n",
        "trainset, valset = random_split(trainset_filtered, [train_size, val_size])\n",
        "\n",
        "# Crear los DataLoaders\n",
        "batch_size = 64\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Tamaño del set de entrenamiento: {len(trainset)} imágenes')\n",
        "print(f'Tamaño del set de validación: {len(valset)} imágenes')\n",
        "print(f'Tamaño del set de test: {len(testset)} imágenes')\n",
        "\n",
        "class_names = ['pájaro', 'gato']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE5j_klRl3_I"
      },
      "source": [
        "### Visualización de Datos\n",
        "Mostramos algunas imágenes para verificar que el filtrado y la carga se han realizado correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Iwfk9fNl3_I"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Desnormalizar\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Obtener un batch de imágenes de entrenamiento\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Mostrar imágenes\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "# Imprimir etiquetas\n",
        "print(' '.join(f'{class_names[labels[j]]:5s}' for j in range(8)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft61eaChl3_J"
      },
      "source": [
        "## 3. Definición del Modelo (LeNet-5)\n",
        "\n",
        "Definimos la arquitectura LeNet-5, adaptándola para las imágenes de CIFAR-10 (32x32x3) y para una clasificación binaria. La última capa tendrá una sola salida, que representará el logit para la clase 'gato'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEHA_09Kl3_J"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # El input es 32x32x3\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # -> 28x28x6\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # -> 14x14x6\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # -> 10x10x16\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # -> 5x5x16\n",
        "\n",
        "        # Las capas fully connected\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # Capa de salida para clasificación binaria (1 logit)\n",
        "        self.fc3 = nn.Linear(84, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = LeNet5().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VSNoDotl3_K"
      },
      "source": [
        "## 4. Entrenamiento del Modelo\n",
        "\n",
        "Definimos la función de pérdida y el optimizador. Usaremos `BCEWithLogitsLoss`, que es adecuada para clasificación binaria y numéricamente estable, ya que combina una sigmoide con la Binary Cross Entropy.\n",
        "\n",
        "Luego, creamos el bucle de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIqjsLTml3_L"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs}'), 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyuGACXKl3_L"
      },
      "source": [
        "## 5. Evaluación de la Calibración del Modelo Original\n",
        "\n",
        "Ahora que el modelo está entrenado, evaluaremos su rendimiento y, lo más importante, su calibración. Para ello, necesitamos obtener las probabilidades predichas y las etiquetas reales del conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkh2JnUUl3_M"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "            probs = torch.sigmoid(logits) # Convertir logits a probabilidades\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu.numpy().flatten())\n",
        "            all_logits.extend(logits.cpu().numpy().flatten())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_probs), np.array(all_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uei1xLHSl3_N"
      },
      "outputs": [],
      "source": [
        "# Obtener predicciones del conjunto de test\n",
        "true_labels, probs_uncalibrated, logits_uncalibrated = get_predictions(net, testloader)\n",
        "\n",
        "# Calcular accuracy\n",
        "preds = (probs_uncalibrated > 0.5).astype(int)\n",
        "accuracy = np.mean(preds == true_labels)\n",
        "print(f'Accuracy en el test set: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Dk_PCql3_N"
      },
      "source": [
        "### Diagrama de Fiabilidad y ECE\n",
        "\n",
        "El diagrama de fiabilidad (reliability diagram) nos permite visualizar qué tan bien calibrado está un modelo. Compara la confianza media de las predicciones (eje x) con la precisión real de esas predicciones (eje y). En un modelo perfectamente calibrado, la gráfica sería la línea diagonal y=x.\n",
        "\n",
        "El **Expected Calibration Error (ECE)** cuantifica esta desviación. Es la diferencia media ponderada entre la precisión y la confianza en varios \"bins\" o intervalos de confianza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQBgU-rzl3_N"
      },
      "outputs": [],
      "source": [
        "def plot_reliability_diagram(true_labels, probs, n_bins=10):\n",
        "    \"\"\"Plots a reliability diagram and calculates ECE.\"\"\"\n",
        "    prob_true, prob_pred = calibration_curve(true_labels, probs, n_bins=n_bins, strategy='uniform')\n",
        "\n",
        "    # Calcular ECE\n",
        "    bin_counts, _ = np.histogram(probs, n_bins, range=(0, 1))\n",
        "    bin_weights = bin_counts / len(probs)\n",
        "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
        "\n",
        "    # Dibujar las barras de confianza vs. precisión\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "\n",
        "    plt.bar(bin_centers, prob_true, width=1.0/n_bins, edgecolor='black', alpha=0.6, label='Model output')\n",
        "\n",
        "    plt.xlabel('Confidence (Predicted Probability)')\n",
        "    plt.ylabel('Accuracy (True Probability in bin)')\n",
        "    plt.title(f'Reliability Diagram (ECE = {ece:.4f})')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "    return ece\n",
        "\n",
        "print(\"Resultados del modelo sin calibrar:\")\n",
        "ece_uncalibrated = plot_reliability_diagram(true_labels, probs_uncalibrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEZEzgN1l3_O"
      },
      "source": [
        "## 6. Implementación de Temperature Scaling\n",
        "\n",
        "Temperature Scaling es una técnica de calibración post-procesamiento muy simple. Consiste en escalar los logits del modelo por un parámetro `a` (o dividirlos por una \"temperatura\" `T`, donde `a = 1/T`) antes de aplicar la función sigmoide.\n",
        "\n",
        "$$ \\text{logit}_{\\text{scaled}} = a \\cdot \\text{logit}_{\\text{original}} $$\n",
        "\n",
        "El parámetro `a` se optimiza para minimizar la Negative Log-Likelihood (NLL) o, en nuestro caso, la `BCEWithLogitsLoss`, en un **conjunto de validación**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D78YNF6Hl3_O"
      },
      "outputs": [],
      "source": [
        "# Primero, obtenemos los logits y etiquetas del conjunto de validación\n",
        "val_labels, _, val_logits = get_predictions(net, valloader)\n",
        "\n",
        "# Convertimos a tensores de PyTorch para la optimización\n",
        "val_logits_tensor = torch.tensor(val_logits, dtype=torch.float32, device=device)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32, device=device).view(-1, 1)\n",
        "\n",
        "# Definimos 'a' como un parámetro entrenable\n",
        "a = nn.Parameter(torch.ones(1, device=device))\n",
        "\n",
        "# Usamos un optimizador para encontrar el mejor 'a'\n",
        "optimizer_a = optim.LBFGS([a], lr=0.01, max_iter=50)\n",
        "\n",
        "def eval_a():\n",
        "    optimizer_a.zero_grad()\n",
        "    loss = criterion(val_logits_tensor * a, val_labels_tensor)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "optimizer_a.step(eval_a)\n",
        "\n",
        "optimal_a = a.item()\n",
        "print(f'El valor óptimo para \\'a\\' es: {optimal_a:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFYOlWADl3_P"
      },
      "source": [
        "### Evaluación de la Calibración después de Temperature Scaling\n",
        "\n",
        "Con el valor óptimo de `a`, escalamos los logits del conjunto de test y volvemos a calcular las probabilidades. Luego, generamos el nuevo diagrama de fiabilidad y el ECE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd6ko4nxl3_Q"
      },
      "outputs": [],
      "source": [
        "# Aplicar el escalado a los logits del conjunto de test\n",
        "logits_calibrated = logits_uncalibrated * optimal_a\n",
        "\n",
        "# Recalcular las probabilidades\n",
        "probs_calibrated = 1 / (1 + np.exp(-logits_calibrated))\n",
        "\n",
        "print(\"Resultados del modelo con Temperature Scaling:\")\n",
        "ece_calibrated = plot_reliability_diagram(true_labels, probs_calibrated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeqXHEJ0l3_Q"
      },
      "source": [
        "## 7. Estudio del Efecto del Parámetro `a`\n",
        "\n",
        "Vamos a visualizar cómo diferentes valores de `a` afectan a las probabilidades y al ECE. Un valor de `a < 1` (T > 1) \"suaviza\" las predicciones, haciéndolas menos confiadas. Un valor de `a > 1` (T < 1) las hace más \"puntiagudas\" o confiadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J008YPIbl3_Q"
      },
      "outputs": [],
      "source": [
        "a_values = [0.5, optimal_a, 1.5] # a < 1, a_optimo, a > 1\n",
        "eces = []\n",
        "\n",
        "for a_val in a_values:\n",
        "    temp_probs = 1 / (1 + np.exp(-(logits_uncalibrated * a_val)))\n",
        "    ece = plot_reliability_diagram(true_labels, temp_probs)\n",
        "    print(f'Para a = {a_val:.4f}, ECE = {ece:.4f}')\n",
        "    eces.append(ece)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c06_YSN4l3_R"
      },
      "source": [
        "## 8. (Opcional) Experimento con un Modelo Pre-entrenado\n",
        "\n",
        "En esta sección, se repetiría el proceso utilizando un modelo más grande y complejo, como ResNet18, pre-entrenado en ImageNet.\n",
        "\n",
        "**Pasos a seguir:**\n",
        "1.  Cargar un modelo `resnet18` pre-entrenado desde `torchvision.models`.\n",
        "2.  Congelar los pesos de todas las capas (`param.requires_grad = False`).\n",
        "3.  Reemplazar la última capa (`fc`) por una nueva capa lineal con una sola salida para nuestra clasificación binaria. Esta será la única capa que se entrene.\n",
        "4.  Ajustar las transformaciones de las imágenes (resize a 224x224 y normalización con las medias y desviaciones de ImageNet).\n",
        "5.  Re-entrenar (fine-tuning) el modelo solo en la nueva capa.\n",
        "6.  Repetir el análisis de calibración (diagrama de fiabilidad, ECE) antes y después de aplicar Temperature Scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6YK-6j0l3_S"
      },
      "outputs": [],
      "source": [
        "# --- CELDA DE CÓDIGO PARA LA PARTE OPCIONAL ---\n",
        "# Aquí iría la implementación de la parte opcional del proyecto.\n",
        "# Puedes descomentar y completar las siguientes líneas como guía.\n",
        "\n",
        "# 1. Cargar modelo pre-entrenado\n",
        "# model_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# 2. Congelar pesos\n",
        "# for param in model_resnet.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# 3. Reemplazar la última capa\n",
        "# num_ftrs = model_resnet.fc.in_features\n",
        "# model_resnet.fc = nn.Linear(num_ftrs, 1)\n",
        "# model_resnet = model_resnet.to(device)\n",
        "\n",
        "# 4. Ajustar transformaciones (requeriría nuevos DataLoaders)\n",
        "# ...\n",
        "\n",
        "# 5. Entrenar y evaluar\n",
        "# ...\n",
        "\n",
        "print(\"Sección opcional. Implementar aquí.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}